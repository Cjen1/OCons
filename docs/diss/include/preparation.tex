\chapter{Preparation}

%Short introduction of what we're going to cover in preparation. All the stuff that was considered %before development began. \\

\section{Theotrical background}

\subsection{Assumptions of the environment}
When considering developing a system with distributed consensus, it is necessary to consider the assumptions made in the environment in which such a system will operate. This is to ensure there is enough functionality embedded in the conesnsus system to ensure that consensus is reached under a given set of assumptions. \\

A \emph{process} (or networked process) is an instance of the program running on a networked machine in a distributed system. Assumptions of these processes that participate in the system:

\begin{itemize}
  \item Processes can undergo \emph{crash failures}. A crash failure is defined as a process terminating but not entering an invalid state.
  \item Processes may recover from crash failures. They can rejoin the system in some valid state. 
  \item Processes operate at arbitrary speeds. This cannot be distinguished by other processes from arbitrarily long delays in the network.
\end{itemize}

Assumptions of the network in which these processes communicate:

\begin{itemize}
  \item All processes can communicate with one another.
  \item The network environment is \emph{asynchronous}. That is, messages may take an arbitrarily long time to be delivered.
  \item Messages may be re-ordered upon delivery.
  \item Messages may be duplicated in the network.
  \item Messages may be dropped from the network.
  \item Messages are not corrupted or modified in the network.
  \item A message that is received by one process was, at some point in the past, sent by another process.
\end{itemize}

% OLD LABELLED BULLETS
%\storestyleof{itemize}
%\begin{listliketab}
%\noindent
%\begin{tabularx}{\linewidth}{@{}LXl@{}}
%    \textbullet & All processes are connected on the network. & (N1) \\
%     \textbullet & The network environment is \emph{asynchronous}. That is, packets may take an arbitrarily long time to be delivered. & (N2) \\
%       \textbullet & Packets may be re-ordered upon delivery, duplicated or dropped from the network. & (N3) \\
%  \textbullet & Packets are not corrupted or modified in the network. & (N4) \\
%  \textbullet & A packet that is received by one process was, at some point in the past, sent by another process. & (N5)
%  \end{tabularx}
%\end{listliketab}

The last two assumptions assume a system that does not tolerate what are known generally as \emph{Byzantine failures}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Aim of consensus}

With distributed consensus, we wish for a network of processes to agree on some value. In consensus algorithms it is assumed that processes can somehow propose values to one participating processes. The goal of distributed consensus is, given a number of processes that can each propose some value $v$, that one of the proposed values is chosen. This is the \emph{single-decree} case, that is only one value is proposed by each process and only one is chosen. \\

In the \emph{multi-decree} case, agreement is reached over a sequence of values. That is, each process will propose a sequence of valus $v_1, v_2, \ldots, v_n$ and the role of the consensus protocol is to have the system choose one such sequence from all those proposed. This multi-decree case allows for the state machine replication technique to be employed to replicate an application across a number of machines in a distributed system.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{State machine replication}


A desirable goal of distributed computing is to replicate an application across a number of machines so that each \emph{replica} has the same strongly-consistent view of the application's state. This technique is referred to as State machine replication (SMR); it leads to both for increased fault tolerance and higher availability. Multi-decree consensus protocols provide a primitive by which an application (that behaves like a state machine) can be replicated. \\

Each process participating in the consensus protocol runs the replicated state machine application, with each process starting in the same state. Then by treating the values proposed in the consensus protocol as \emph{commands} to perform a state transition, then by running a consensus protocol each process will receive the same serialized sequence of commands $c_1, c_2, \ldots, c_n$. These commands are treated as commands to perform a state transition and as such each process perform the same sequence of transitions from the same starting state and thus be a replica of the the state machine application. \\

Before considering how to implement SMR in the multi-decree case, it is useful to examine how Paxos operates in the simpler single-decree case. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Single-decree Paxos}

\begin{table}
  \centering
  \begin{tabular}{l|p{8cm}|c}
    Role & Purpose & Number required \\ \hline \hline
    Proposer & Propose values to acceptors. & $f+1$ \\
                     & Send prepare requests with proposal numbers. & \\ [.5\normalbaselineskip] \hline
    Acceptor & Decide whether to \emph{adopt} a proposal based on its proposal number & $2f+1$ \\
                     & Decide whether to \emph{accept} a proposal based on a higher numbered proposal having arriving. & \\ [.5\normalbaselineskip] \hline
    Learner & Learn value chosen by majority of acceptors & $f + 1$ \\ [.5\normalbaselineskip]
  \end{tabular}
  \caption{Summary of the roles in single-decree Paxos. In this description the system can tolerate the failure of up to $f$ of each given role.}
\label{table:role-summary}
\end{table}

Single-decree Paxos is the variant of the algorithm that allows for a single value to be chosen from a set of proposals and provides a foundation for the multi-decree case that will be considered next. The terminology used here follows Lamport's paper \cite{paxos-made-simple} describing the single-decree protocol in simple terms. Processes take the roles of \emph{proposers}, \emph{acceptors} and \emph{learners}, each of which has a designated task in the Paxos algorithm. In reality these roles are often co-located within a single process but it is simply to consider each separately.  The prupose of each role and the number of each role required to tolerate $f$ failures is summarised in Table \ref{table:role-summary}.  \\

Proposers that wish to propose a value $v$ submit proposals of the form $\left(n,v\right)$, where $n \in \mathbb{N}$ is called a proposal number. Each proposer may propose one proposal at a time and may only use strictly increasing proposal numbers for each proposal. Furthermore, each proposer must use a disjoint set of proposal numbers. The Paxos algorithm is divided into a number of stages described below. \\

\textbf{Phase 1a (Prepare phase)} A proposer wishing to propose a value first sends a \texttt{prepare($n$)} message to a majority of the set of acceptors, where $n$ is the highest proposal number it has used so far. \\

\textbf{Phase 1b (Promise phase)} In this phase an acceptor receiving a \texttt{prepare($n$)} message must decide whether or not to \emph{adopt} this proposal number. Adopting a proposal number is the act of promising not to accept a future proposal number $n'$ such that $n' < n$. The acceptor will adopt $n$ if it is the highest proposal number it has received thus far, in which case it will reply to the proposer with a \texttt{promise($n''$,$v$)} message, where $n''$ is the highest proposal number it has previously accepted and $v$ is the corresponding proposal's value. Otherwise, it can simply ignore the proposer or send a \texttt{NACK} message so the proposer can abandon the proposal. \\

\textbf{Phase 2a (Accept phase)} Upon receipt of a \texttt{promise($n$,$v$)} message from a majority of the set of acceptors, the proposer replies to each with an \texttt{accept($n'$,$v'$)}, where $n'$ is the highest proposal number returned by the acceptors in the promise phase and $v'$ is its corresponding value. \\

\textbf{Phase 2b (Commit phase)} An acceptor receiving a \texttt{accept($n$,$v$)} message from a proposer will decide whether to commit the proposal for $v$. If the acceptor hasn't made a promise to adopt a proposal number higher than $n$, then it will commit $v$, otherwise it will ignore this message or send a \texttt{NACK} to the proposer. \\

Once this process is completed, a majority of the acceptors will have chosen the same proposed value. Learners are required to learn what value was chosen by the majority. A number of different methods can be employed to deliver this information. Acceptors can, on choosing a value to accept, broadcast their decision to the set of learners. An alternative method is to have a distringuished learner (or small subset of) that are sent all decisions which then forward onto the set of learners when they have learned the majority. \\

{\color{blue}Talk about some simple examples with corresponding timing diagrams.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Multi-decree Paxos}

Single-decree Paxos can be naively extended by allowing proposers to propose values one at a time. However, this is wasteful as it requires that proposers send \texttt{prepare} messages for each proposal they wish to make. A number of optimisations and extensions can be put in place to increase the efficiency of the system. The system here primarily follows {\color{red}PAXOS MADE MODERATELY COMPLEX} and introduces different types of nodes. {\color{red}Also discussed here is how to extend the system to use state machine replication}. The new roles and their correspondence to the single-decree roles are summarised in Table \ref{table:multi-role-summary}. \\

{\color{blue}Diagram showing the communication pattern of nodes in Mutli-Paxos. Contrast with the single-decree case.} \\

Clients and replicas are introduced to provide a means of implementing the replicated state machine.

\begin{table}
\centering
\begin{tabular}{ l | p{8cm} | c }
   Role & Purpose & Number required \\ \hline \hline
   Client & Send commands to replicas and receive responses & $ N/A $ \\ \hline
  
   Replica & Receive requests from clients. &  \\
                 & Serialize proposals and send to leaders. & $f + 1$ \\
                 & Receive decisions and apply to the replicated application state. & \\
                 & Handle reconfiguration of set of leaders & \\ \hline
  
  Leader & Request acceptors adopt ballots. & $f + 1$ \\
               & ... & \\
               & ... &  \\ \hline
               
  Acceptor & Fault tolerant distributed memory. Voting protocol. & $2f + 1$ \\
                   & ... & \\
                   & ... &  \\ \hline
\end{tabular}
\caption{Summary of the roles in Multi Paxos. In this description the system can tolerate the failure of up to $f$ of each given role. Note that clients do not explicitly participate in the protocol and so there is no requirement on any number being live at any given time.}
\label{table:multi-role-summary}
\end{table}


\subsubsection{Clients}

The purpose of clients is to allow for commands to be sent externally to the system which can then be formed into proposals internally. This allows the system to behave in a manner more like that of a typically deployed distributed system (with a client / server architecture) and provides a degree of failure transparency. \\

A command $c$ takes the form $\left( \kappa, \mathrm{\emph{cid}}, \mathrm{\emph{op}} \right)$, where $\kappa$ is a unique identifier for the client, \emph{cid} is a unique identifier for the client's sent commands and \emph{op} is the operation the command should perform. Clients broadcast a \texttt{request($c$)} message to the replicas and each is issued a \texttt{response(}\emph{cid}\texttt{,}\emph{result}\texttt{)} when consensus is reached and it has been applied to each replica's application state.


\subsubsection{Replicas}

Replicas receive commands and attempt to serialize them by converting each command $c$ into a proposal $\left(s,c\right)$, where $s \in \mathbb{N}$ is a slot number. The slot number describes ordering of the sequence in which the commands should be committed; this is not to be confused with the proposal number $n$ in the single-decree protocol. \\

Different replicas may form different sequences of proposals and so broadcasts a \texttt{propose($s$,$c$)} message to the set of leaders and awaits a \texttt{decision($s'$,$c'$)} message. The resulting decision may differ in its slot number and so the replica may have to re-propose a command it has proposed for the decided slot. Upon receipt of decisions the replica will applying the associated operation to the application state, maintaining the replicated application. \\

{\color{red}(Also reconfigurations)} \\

{\color{blue}Diagram showing message flow between clients and replicas to clarify the last points.}


\subsubsection{Ballots and pvalues}

Explain ballots.

\begin{itemize}
  \item A ballot may map a command to multiple slots.
  \item A slot may be mapped to multiple ballots.
  \item Leaders can attempt to secure adoption of multiple ballots concurrently.
\end{itemize} 

Ballot numbers are either pairs $\left(r, \lambda\right)$ (where $r \in \mathbb{N}$ is called a round number and $\lambda$ is a leader's unique identifier) or $\bot$, a specially designated least ballot number. \\

A \emph{pvalue} is a triple $\left(b, s, c \right)$ consisting of a ballot number, a slot number and a command. These are analogous to to the $\left( n, v \right)$ pairs used in the single-decree case. In the single-decree case, we required that each proposer used a disjoint subset of $\mathbb{N}$ for their proposal numbers. We can avoid this requirement as each ballot number encodes the identifier of the leader directly in its ballot number (i.e. no two leaders can generate equal ballot numbers). \\

We require ballot numbers to be totally ordered so that acceptors can compare which ballot number is less than another when choosing whether to adopt or accept. Letting $\mathcal{B}$ denote the set of all ballot numbers, we define the relation $\leq \ \in \mathcal{B} \times \mathcal{B}$ which satisfies the following two conditions:

\begin{gather}
  \forall \left( n, \lambda \right), \left( n', \lambda' \right) \in \mathcal{B} .
 \ \left( n, \lambda \right) \leq \left( n', \lambda' \right) \iff
   \left ( n \leq n' \right) \vee \left( n = n' \wedge \lambda \leq \lambda' \right) \\
\forall \ b \in \mathcal{B} \ . \ \bot \leq b
\end{gather}  

Note this implies that we require leader identifiers be equipped with a total order relation as well.


\subsubsection{Quorums}

Explanation of quorum systems. \\

{\color{red}
Let $\mathcal{Q}$ be the set of all quorums of acceptors, that is $\mathcal{Q} = \mathcal{P \left( \mathcal{A} \right)} $. Quorums are \emph{valid} if they share at least one common member, that is

$$\forall Q_1, Q_2 \in \mathcal{Q} \ . \ Q_1 \cap Q_2 \neq \emptyset$$

Hence we can use a majority quorum by requiring that $|Q_1| = |\mathcal{A}| \ / \ 2$.

Need to really think about these quorum systems.}



\subsubsection{The Synod Protocol}

The synod protocol is the protocol undertaken by the set of leaders and acceptors in order to decide which command is committed to which slot. The protocol proceeds in two phases similarly to the single-decree case except now leaders and acceptors operate over pvalues. \\

In the absence of receiving any \texttt{propose($s$,$c$)} messages from replicas, leaders attempt to secure an initial ballot with ballot number $\left(0, \lambda_i)$, where $\lambda_i$ is the identifier of the {\color{red}ith} leader. They do this by broadcasting a \texttt{phase1a($b$)} message in the same format as that below. \\

\textbf{Phase 1a} Leaders attempt to secure an initial ballot by broadcasting \texttt{phase1a($\left( n, \lambda \right)$)} message.\\

\textbf{Phase 1b} Acceptors receiving a \texttt{phase1a($\left( n, \lambda \right)$)} compare $\left( n, \lambda \right)$ to the highest ballot number they have adopted thus far. This is initially $\bot$ so acceptors will adopt the first ballot number they receive automatically. If $b \leq \left( n, \lambda \right)$, then the acceptor will adopt this new ballot number. In either case, the acceptor will reply with a \texttt{phase1b($b'$, pvals) message.

\textbf{Phase 2a} ... \\

\textbf{Phase 2b} ... \\

Go on to summarise and clear up any other points. Then reference a diagram showing message flow in the case of the synod protocol. \\

Touch on duelling proposals and the impossibility result. Include a duelling proposals timing diagram.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Requirements}

\subsection{System requirements}

As noted previously, there are a number of papers that describe the Paxos protocol and its variants. Many of these sources present the algorithm in a different format and nearly all of them present it in a theoretical setting, with little concern for functionality that is generally required in software. Implementation details such as how each of the participating process knows how to address one another are often entirely overlooked. Hence, in developing an implementation of Multi-Paxos, it is necessary to formally recognise the theoretical requirements from the literature as well as identify the additional functionality required to realise the algorithm as a functioning piece of software. \\

It is therefore necessary to perform a \emph{requirements analysis} of the final system. This was performed by collecting information from the literature on how the algorithm is presented and also considering all of the necessary functionality required to implement such an algorithm. For example, in the Paxos Made Moderately Complex paper \cite{VanRenesse:2015:PMM:2737799.2673577}, processes are described as being able to send messages between one another. This of course presents the requirement that {\color{red}we} implement some sort of messaging primitive that operates on top of the unreliable network running typical IP technology. The final system requirements are listed below. \\

\textbf{System-like requirements}
\begin{itemize}
  \item Command line interface
  \item Select type of process and proceed
  \item Configuration files and a means of nodes taking a specific config
  \item Ability to log salient information to disk, adjust granularity of logs, etc.
  \item {\color{red}Persist data to disk for crash recovery.}
\end{itemize}

\textbf{Messaging subsystem}
\begin{itemize}
  \item Provide an abstraction over the network so processes can send messages, rather than worrying about functionality of network.
  \item Send messages as per the requirements of the consensus algorithm
  \item Provide semantics around failures we would expect in this case.
\end{itemize}

\textbf{Consensus algorithm requirements}
\begin{itemize}
  \item Strongly replicated key-value store as application
  \item Clients send messages with commands for this application and receive responses.
  \item Replicas.
  \item Leaders.
  \item Acceptors.
  \item Quorums.
  \item {\color{red}Reconfigurations.}
\end{itemize}

\subsection{Testing and evaluation requirements}

It is crucial that there is a system to check that the algorithm behaves as expected in the environement described in the assumptions. \\

Talk about using Mininet and the built in scripting ability to provide such a test harness. \\

\textbf{Test harness}
\begin{itemize}
  \item ...
  \item ...
  \item ...
\end{itemize}

{\color{green} \subsection{Extension elements}
Talk about FPaxos here (when I've actually built it.)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Software engineering}

\subsection{Methodology}
\begin{itemize}
  \item Discuss the software development methodology (something along the lines of Agile I imagine)
  \item Describe the testing strategy. Talk about OUnit for unit testing functions as simple units and then more thorough and complete tests for correct funtionality at the cluster-level - (these can either be Mininet scripts or something else entirely...)
\end{itemize}

\subsection{Libraries}

\begin{itemize}
  \item Mention OPAM package manager and how it is used for installing / managing libraries and dependencies
  \item Discuss the licenses of these packages (professional practice)
  \item Give particular explanation to Capn'Proto and Lwt as these are the most interesting and pervasive libraries used in the project. Save technical details for implementation.
  \item Remark briefly about using Core as standard library replacement, Yojson for some serialization, OUnit for unit testing, some lib for command line parsing.
\end{itemize}

\subsection{Compiler}
JBuilder {\color{red}(Renamed Dune)}\footnote{https://github.com/ocaml/dune} was used as the compiler for the project. This was chosen as configuration files are presented in a simple S-Expression syntax. It also allowed for Cap'n Proto schema files to be re-compiled into OCaml boilerplate upon each re-build of the project. It also automates the generation of \texttt{.merlin} files for auto-completion in Vim (see the section on Tools).

\subsection{Tools}

Choice of tools is important in being productive in a language. Vim was chosen. Merlin was syntax highlighting. 

Git was used both for version control and for backups with Github. Additionally, backups were performed by having the project directory located in a Google Drive. 

\begin{itemize}
  \item Vim with Merlin for type inference whilst editing.
  \item Git for version control, Github (and Google Drive) for backups.
  \item Mininet (with VirtualBox) for evaluation.
\end{itemize}


